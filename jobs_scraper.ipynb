{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SG Indeed web scraper\n",
        "## Scrapes [sg.indeed.com](https://sg.indeed.com/) for job postings from Singapore\n",
        "<img src='https://d34k7i5akwhqbd.cloudfront.net/allspark/static/images/indeed-share-image-9581a8.png' alt='Indeed Logo' width=\"400\">\n",
        "\n",
        "### Overview\n",
        "There is a lack of publically available job datasets in Singapore context.\n",
        "\n",
        "This tool scrapes raw data from [sg.indeed.com](https://sg.indeed.com/). With each job posting, we retrieve:\n",
        "\n",
        "\n",
        "*   Job title\n",
        "*   Job description\n",
        "*   URL to job posting\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Datasets\n",
        "The following data sets have been generated with this tool:"
      ],
      "metadata": {
        "id": "C--rNVXGdAfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start scraping web pages"
      ],
      "metadata": {
        "id": "nqKBklcYmJnm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "za7FLMvNMn-1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from time import sleep\n",
        "from random import random\n",
        "\n",
        "def scrape(job_title, count):\n",
        "    job_postings = []\n",
        "    job_title = job_title.replace(' ', '+')\n",
        "    \n",
        "    for i in range(count // 15):\n",
        "        page = \"https://sg.indeed.com/jobs?q={}&start={}\".format(job_title, i*10)\n",
        "        page = requests.get(page)\n",
        "\n",
        "        soup = BeautifulSoup(page.content)\n",
        "        jobs = soup.find(\"ul\", {\"class\": \"jobsearch-ResultsList\"}).findAll('li',recursive=False)\n",
        "        del jobs[17]\n",
        "        del jobs[11]\n",
        "        del jobs[5]\n",
        "\n",
        "        for job in jobs:\n",
        "            title = job.find(\"h2\", {\"class\": \"jobTitle\"}).a\n",
        "            url = \"https://sg.indeed.com\"+title[\"href\"]\n",
        "            job_soup = BeautifulSoup(requests.get(url).content,\"lxml\")\n",
        "            job_postings.append((url, job_soup))\n",
        "            sleep(random()*2+1)\n",
        "\n",
        "        print(f\"Found {len(job_postings)}/{count} job postings.\")\n",
        "\n",
        "    return job_postings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to start scraping.\n",
        "\n",
        "The function takes 2 arguments:\n",
        "\n",
        "    job_title: str, The search term used to find job postings\n",
        "\n",
        "    count: int, The number of postings to scrape\n"
      ],
      "metadata": {
        "id": "d1q1zqn6l-aq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B99TKjeFNK3W",
        "outputId": "59ea4025-fa55-4d60-b2b1-c2b7a2b758f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15/600 job postings.\n",
            "Found 30/600 job postings.\n",
            "Found 45/600 job postings.\n",
            "Found 60/600 job postings.\n",
            "Found 75/600 job postings.\n",
            "Found 90/600 job postings.\n",
            "Found 105/600 job postings.\n",
            "Found 120/600 job postings.\n",
            "Found 135/600 job postings.\n",
            "Found 150/600 job postings.\n",
            "Found 165/600 job postings.\n",
            "Found 180/600 job postings.\n",
            "Found 195/600 job postings.\n",
            "Found 210/600 job postings.\n",
            "Found 225/600 job postings.\n",
            "Found 240/600 job postings.\n",
            "Found 255/600 job postings.\n",
            "Found 270/600 job postings.\n",
            "Found 285/600 job postings.\n",
            "Found 300/600 job postings.\n",
            "Found 315/600 job postings.\n",
            "Found 330/600 job postings.\n",
            "Found 345/600 job postings.\n",
            "Found 360/600 job postings.\n",
            "Found 375/600 job postings.\n",
            "Found 390/600 job postings.\n",
            "Found 405/600 job postings.\n",
            "Found 420/600 job postings.\n",
            "Found 435/600 job postings.\n",
            "Found 450/600 job postings.\n",
            "Found 465/600 job postings.\n",
            "Found 480/600 job postings.\n",
            "Found 495/600 job postings.\n",
            "Found 510/600 job postings.\n",
            "Found 525/600 job postings.\n",
            "Found 540/600 job postings.\n",
            "Found 555/600 job postings.\n",
            "Found 570/600 job postings.\n",
            "Found 585/600 job postings.\n",
            "Found 600/600 job postings.\n"
          ]
        }
      ],
      "source": [
        "raw_text = scrape(\"software engineer\", 600)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract data from pages"
      ],
      "metadata": {
        "id": "1o-x-QkZoK2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "raw_data = {\"URL\": [], \"Job Title\": [], \"Job Description\": []}\n",
        "for url, job in raw_text:\n",
        "    title = job.find(\"h1\", {\"class\": \"jobsearch-JobInfoHeader-title\"}).get_text()\n",
        "    description = job.find(\"div\", {\"class\": \"jobsearch-JobComponent-description\"}).get_text()\n",
        "\n",
        "    raw_data[\"URL\"].append(url)\n",
        "    raw_data[\"Job Title\"].append(title)\n",
        "    raw_data[\"Job Description\"].append(description)\n",
        "\n",
        "raw_data = pd.DataFrame(raw_data)"
      ],
      "metadata": {
        "id": "t-Tv0ehgbzUh"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export to google drive"
      ],
      "metadata": {
        "id": "7xmjD_Zu6fUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILENAME = 'datascientist.csv' # edit this line to rename file\n",
        "raw_data.to_csv(f'/content/drive/My Drive/{FILENAME}', index=False)\n",
        "\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4-hFGu06kG4",
        "outputId": "ce6172e9-f53d-47db-a9d1-5e51fe02a0df"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "jobs_scraper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}